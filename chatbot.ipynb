{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2548e3c8",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project builds a question answering chatbot using Retrieval Augmented Generation (RAG) on BBC News article summaries. The chatbot can answer questions about news topics by finding relevant article passages and generating natural language responses. The chatbot requires a Python backend with a local LLM and vector database running on a server, but GitHub Pages only hosts static files and can't run code.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset consists of BBC News article summaries from Kaggle, organized into five category labels: business, entertainment, politics, sport, and tech. This structure mirrors the movie review dataset from class, which used text files with positive and negative labels, but expands to multiple topic categories.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The project uses a virtual Python environment dedicated to RAG development. A virtual environment isolates project dependencies, similar to how rooms in a house keep different activities separate. The local large language model runs through Ollama, an interface that connects to the phi3 mini model for generating responses.\n",
    "\n",
    "## How RAG Works\n",
    "\n",
    "The RAG pipeline follows three main steps. First, the Sentence Transformer package converts all news article text into numerical vectors that capture semantic meaning. These vectors are stored in Qdrant, a vector database loaded as a Python package. When a user asks a question, the same Sentence Transformer converts the question into a vector. Qdrant then finds stored vectors with the highest similarity scores to the question vector, returning the top matching text passages as answer candidates. Finally, both the original question and the candidate passages are sent to the LLM, which generates a coherent answer based on the retrieved context.\n",
    "\n",
    "## Results\n",
    "\n",
    "The chatbot successfully answers questions across all five news categories, drawing on relevant article content to provide informed responses about trends in technology, politics, sports, and other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916063a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd4bd-26ea-47cb-adc3-ba326a7d2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os, glob, random, re, html\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "# Progress / arrays\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Qdrant (embedded)\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456927bb-120e-4649-897d-34ba4980be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local LLM via Ollama\n",
    "import requests\n",
    "# ----------- Config ----------\n",
    "DATA_ROOT = Path(r\"C:\\Users\\spink\\OneDrive\\Desktop\\Database Managment\\archive (1)\\BBC News Summary\\Summaries\") \n",
    "# because we're inside the aclImdb folder\n",
    "SAMPLE_N = 1000 # per label (pos/neg)\n",
    "COLLECTION = \"bbc_news_rag_demo_nb\"\n",
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "OLLAMA_MODEL = \"phi3:mini\"\n",
    "QDRANT_PATH = \"qdrant_data_news_nb\"\n",
    "CHUNK_MAX_CHARS = 600\n",
    "TOP_K = 5\n",
    "SEED = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307b559-5617-4212-97d9-03394207396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r\"<br\\s*/?>\", \" \", s, flags=re.I)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def chunk_text(text: str, max_chars=600) -> List[str]:\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks, cur = [], \"\"\n",
    "    \n",
    "    for s in sents:\n",
    "        if len(cur) + len(s) + 1 <= max_chars:\n",
    "            cur = f\"{cur} {s}\".strip()\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            cur = s\n",
    "    \n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45fd57-aa22-4841-8250-3b59333f1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 250 reviews (pos=50, neg=50).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'The head of Christian Brothers\\' school St Fintian\\'s, Richard Fogarty, said the video implied that the 24-year-old pop star had attended his school and was abused there.McFadden makes claims that he was beaten at his own school in the song\\'s lyrics, saying it had \"cell blocks\".They have said the reference to the school was unintentional and coincidental.The new video of former Westlife singer Brian McFadden has been pulled after a Dublin school complained about being associated with his song Irish Son.Corporal punishment was outlawed in Irish schools in 1982 when McFadden was two years old.St Fintian\\'s High School says it is clearly identified in the video, while McFadden never went there.',\n",
       "  'label': 'entertainment',\n",
       "  'path': 'C:\\\\Users\\\\spink\\\\OneDrive\\\\Desktop\\\\Database Managment\\\\archive (1)\\\\BBC News Summary\\\\Summaries\\\\entertainment\\\\123.txt'},\n",
       " {'text': 'British citizens are being included in the changes after the law lords said the current powers were discriminatory because they could only be used on foreign suspects.He said intercept evidence was only a small part of the case against the men and some of it could not be used because it could put sources\\' lives at risk.Under the proposed changes - prompted by the House of Lords ruling - the home secretary could order British citizens or foreign suspects who could not be deported, to face house arrest or other measures such as restrictions on their movements or limits on their use of telephones and the internet.He said the standard of proof for the new powers would have to be \"very high indeed\" and he asked whether ministers had looked at measures which fitted with human rights laws.The Law Society dubbed Mr Clarke\\'s new proposals an \"abuse of power\".It comes after law lords ruled that the detention of 12 foreign terror suspects without trial breached human rights.UK citizens suspected of involvement in terrorism could face house arrest as part of a series of new measures outlined by the home secretary.There have been calls for the rules for wire-tap and intercept evidence to be allowed to be used in courts but Mr Clarke refused to back that change.Mr Clarke said prosecutions were the government\\'s first preference and promised the powers would only be used in \"serious\" cases, with independent scrutiny from judges.He suggested changing the law to let security-cleared judges view evidence gathered by phone-tapping could allow more terror cases to come to court.Mr Clarke also said intelligence reports showed some British nationals were now playing a more significant role in terror threats.',\n",
       "  'label': 'politics',\n",
       "  'path': 'C:\\\\Users\\\\spink\\\\OneDrive\\\\Desktop\\\\Database Managment\\\\archive (1)\\\\BBC News Summary\\\\Summaries\\\\politics\\\\384.txt'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(r\"C:\\Users\\spink\\OneDrive\\Desktop\\Database Managment\\archive (1)\\BBC News Summary\\Summaries\")\n",
    "\n",
    "SAMPLE_N = 50\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def load_sample_reviews(n_per_label=SAMPLE_N) -> List[Dict]:\n",
    "    rows = []\n",
    "\n",
    "    for label in [\"tech\", \"sport\", \"politics\", \"entertainment\", \"business\"]:\n",
    "        files = glob.glob(str(DATA_ROOT / label / \"*.txt\"))\n",
    "\n",
    "        random.seed(SEED)\n",
    "        random.shuffle(files)\n",
    "\n",
    "        selected = files[:n_per_label]\n",
    "\n",
    "        for f in selected:\n",
    "            txt = Path(f).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            rows.append({\n",
    "                \"text\": clean_text(txt),\n",
    "                \"label\": label,\n",
    "                \"path\": str(f)\n",
    "            })\n",
    "\n",
    "    random.shuffle(rows)\n",
    "    print(f\"Loaded {len(rows)} reviews (pos={SAMPLE_N}, neg={SAMPLE_N}).\")\n",
    "    return rows\n",
    "\n",
    "rows = load_sample_reviews()\n",
    "rows[:2]   # peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59713c37-bbc9-4ceb-baaa-9459ac1526f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|█████████████████████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 2936.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 426\n",
      "\n",
      "Example review path: C:\\Users\\spink\\OneDrive\\Desktop\\Database Managment\\archive (1)\\BBC News Summary\\Summaries\\sport\\362.txt\n",
      "Original review length: 471 characters\n",
      "Number of chunks created: 1\n",
      "\n",
      "--- Chunk 1 ---\n",
      "The Wales Students rugby side has become a casualty of the Welsh Rugby Union's reorganisation at youth level.The secretary of the Welsh Students Rugby Football Union, Reverend Eldon Phillips, said: \"It is a shame that fixtures cannot be maintained this year.The Welsh Students Rugby Football Union feels that it is unable to properly prepare for or stage the matches.But that move has seen the WRU decide to end its funding of representative sides such as Wales Students.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks, meta = [], []\n",
    "random.seed(SEED)\n",
    "\n",
    "for r in tqdm(rows, desc=\"Chunking\"):\n",
    "    text = r.get(\"text\")\n",
    "    if not text:\n",
    "        continue  # skip empty rows\n",
    "\n",
    "    chunks_in_review = chunk_text(text, CHUNK_MAX_CHARS)\n",
    "    if not chunks_in_review:\n",
    "        continue  # skip if function returns None or empty\n",
    "\n",
    "    for j, ch in enumerate(chunks_in_review):\n",
    "        chunks.append(ch)\n",
    "        meta.append({\n",
    "            \"label\": r[\"label\"],\n",
    "            \"source\": r[\"path\"],\n",
    "            \"chunk_id\": j\n",
    "        })\n",
    "\n",
    "print(f\"Total number of chunks created: {len(chunks)}\")\n",
    "\n",
    "# Example: show a sample review and its chunks\n",
    "sample_review = random.choice(rows)\n",
    "sample_chunks = chunk_text(sample_review[\"text\"], CHUNK_MAX_CHARS)\n",
    "\n",
    "print(\"\\nExample review path:\", sample_review[\"path\"])\n",
    "print(f\"Original review length: {len(sample_review['text'])} characters\")\n",
    "print(f\"Number of chunks created: {len(sample_chunks)}\\n\")\n",
    "\n",
    "# Show all chunks with numbering\n",
    "for i, chunk in enumerate(sample_chunks, 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(chunk)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42af1c-9360-4921-a912-fd4efce66f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da890169eb04ffcb4eceb51c07856b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spink\\anaconda3\\envs\\rag_demo\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\spink\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e25a6312d224025abe72493859ced4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3199b134e3d468a88f0caa104b51c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213c55fe3bf84def86e9379019db8aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db946b10856d4d6fa6c81e1a3b00dc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f74afe656c648f09cd84768334aac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb5467b96d64131bad4d832f46ba28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cb39a8295245368bafef73d974aeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6695e138ea8a42efb4f1435edee3d755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d974a4a041d461db613259483ecdb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d22061f5dee49b3ba60af13e14d1e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = SentenceTransformer(EMB_MODEL)\n",
    "EMB_DIM = embedder.get_sentence_embedding_dimension()\n",
    "EMB_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5161336-3b12-44b5-849c-b9296fcd02c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CollectionDescription(name='bbc_news_rag_demo_nb')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(path=QDRANT_PATH)  # embedded, no Docker needed\n",
    "\n",
    "existing = [c.name for c in client.get_collections().collections]\n",
    "\n",
    "if COLLECTION not in existing:\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config=VectorParams(\n",
    "            size=EMB_DIM,\n",
    "            distance=Distance.COSINE\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# quick check\n",
    "client.get_collections().collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fbfb6-b2d5-47dd-9bce-7406cec0df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points in collection before: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding + Upserting:   0%|                                                                                  | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ca9f4d5644f359bbef46e57515fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding + Upserting: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points in collection after: 426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    vecs = embedder.encode(\n",
    "        texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    return vecs.astype(np.float32)\n",
    "\n",
    "# Only ingest if empty\n",
    "info = client.get_collection(COLLECTION)\n",
    "print(\"Points in collection before:\", info.points_count)\n",
    "\n",
    "if info.points_count == 0:\n",
    "    batch = 800\n",
    "    idx = 0\n",
    "\n",
    "    for start in tqdm(range(0, len(chunks), batch), desc=\"Embedding + Upserting\"):\n",
    "        end = min(start + batch, len(chunks))\n",
    "\n",
    "        vecs = embed_texts(chunks[start:end])\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=idx + i,\n",
    "                vector=vecs[i].tolist(),\n",
    "                payload={\"text\": chunks[start + i], **meta[start + i]},\n",
    "            )\n",
    "            for i in range(end - start)\n",
    "        ]\n",
    "\n",
    "        client.upsert(COLLECTION, points=points)\n",
    "        idx += end - start\n",
    "\n",
    "info = client.get_collection(COLLECTION)\n",
    "print(\"Points in collection after:\", info.points_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fa726-b3fc-4ef5-a2a4-3aa3cd1bb5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde0270f3b9541cc863be3bf25947bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0.293178018544397,\n",
       "  'sport',\n",
       "  '\"\"I have to be positive, I still have a few weeks,\" she said. \"But I think there\\'ll be less pressure than last time even...'),\n",
       " (0.2612251127514186,\n",
       "  'entertainment',\n",
       "  'Preview performances of the £3m musical Billy Elliot have been delayed to give the child actors a less arduous rehearsal...'),\n",
       " (0.25846216648855,\n",
       "  'politics',\n",
       "  '\"Mr Howard argued the only test for his policies was whether they were best for Britain.Mr Howard says he will produce a...'),\n",
       " (0.2449469229490941,\n",
       "  'sport',\n",
       "  '\"It\\'s a good way to end the year,\" she said....'),\n",
       " (0.23725151362066826,\n",
       "  'sport',\n",
       "  '\"Campbell said: \"It means a lot to me to go through, it\\'s everything....')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(query: str, top_k=TOP_K):\n",
    "    qv = embed_texts([query])[0].tolist()\n",
    "    res = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=qv,\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return res.points  # list of ScoredPoint\n",
    "\n",
    "\n",
    "# Example search\n",
    "hits = search(\"What do reviewers say about pacing?\")\n",
    "\n",
    "[\n",
    "    (\n",
    "        h.score,\n",
    "        h.payload[\"label\"],\n",
    "        h.payload[\"text\"][:120].replace(\"\\n\", \" \") + \"...\"\n",
    "    )\n",
    "    for h in hits\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97dd14-afdf-4a91-b5b8-7eb0489a0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, hits, max_chars_per_chunk=380):\n",
    "    ctx_blocks = []\n",
    "    \n",
    "    for i, h in enumerate(hits, 1):\n",
    "        txt = h.payload[\"text\"][:max_chars_per_chunk]  # truncate\n",
    "        src = h.payload.get(\"source\", \"unknown\")\n",
    "        ctx_blocks.append(f\"[{i}] {txt}\\n(Source: {src})\")\n",
    "\n",
    "    ctx = \"\\n\\n\".join(ctx_blocks)\n",
    "\n",
    "    return f\"\"\"Answer the question using ONLY the context. Cite sources as [1], [2], etc.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": OLLAMA_MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\"temperature\": 0.2},\n",
    "            },\n",
    "            timeout=120,\n",
    "        )\n",
    "\n",
    "        # If Ollama returned an error JSON, surface it\n",
    "        if r.status_code >= 400:\n",
    "            try:\n",
    "                return f\"[LLM ERROR {r.status_code}] {r.json()}\"\n",
    "            except Exception:\n",
    "                r.raise_for_status()\n",
    "\n",
    "        return r.json().get(\"response\", \"\").strip()\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return (\n",
    "            \"[LLM ERROR] Could not connect to Ollama at http://localhost:11434.\\n\"\n",
    "            \"Ensure Ollama is installed/running and the model is pulled:\\n\"\n",
    "            \"    ollama pull phi3:mini\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[LLM ERROR] {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82256d9c-c1ef-4f21-b6c2-a86b2ae6e1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4fc3861743476a9b7596c6a5032d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Answer ===\n",
      " The provided context does not include specific information about what reviewers complain about regarding pacing in any given work or event. Therefore, I don't know the answer to this question based on these sources.\n",
      "\n",
      "=== Top matches ===\n",
      "[1] (politics) score=0.246 :: \"\"Trust, plain-speaking and straight talking is something which matters so much to me as a politician and as a man that I have decided, of my own volition, to request an independent review of the alle ...\n",
      "[2] (sport) score=0.242 :: \"\"I have to be positive, I still have a few weeks,\" she said. \"But I think there'll be less pressure than last time even if I am champion.\" ...\n",
      "[3] (entertainment) score=0.240 :: Preview performances of the £3m musical Billy Elliot have been delayed to give the child actors a less arduous rehearsal schedule.Director Stephen Daldry made the decision to re-schedule the previews  ...\n",
      "[4] (politics) score=0.234 :: \"Mr Howard argued the only test for his policies was whether they were best for Britain.Mr Howard says he will produce a Timetable for Action so people can hold him to account but on issues like taxat ...\n",
      "[5] (politics) score=0.212 :: The councils' umbrella organisation Cosla, which provided BBC Scotland with the indicative figures for next year, warned that councils would face a continuous struggle to maintain services.The finance ...\n"
     ]
    }
   ],
   "source": [
    "question = \"What do reviewers complain about regarding pacing?\"\n",
    "\n",
    "hits = search(question, top_k=5)\n",
    "prompt = build_prompt(question, hits)\n",
    "answer = call_llm(prompt)\n",
    "\n",
    "print(\"=== Answer ===\\n\", answer)\n",
    "\n",
    "print(\"\\n=== Top matches ===\")\n",
    "for i, h in enumerate(hits, 1):\n",
    "    snip = h.payload[\"text\"][:200].replace(\"\\n\", \" \")\n",
    "    print(f\"[{i}] ({h.payload['label']}) score={h.score:.3f} :: {snip} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae70e07-f3dc-4c66-9128-acf670a7f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Q: What are some trends in tech?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09e72973c73412398a416fb771ea168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid devices and portable digital music players are some trends in tech, as they combine multimedia functions or offer on-the-go entertainment options [1][3]. Additionally, the growth of broadband services like voice and TV over the internet presents new challenges for network infrastructure to support these demands [5].\n",
      "\n",
      "==============================\n",
      "Q: What are some trends in politics?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e532496f76614b94a585407f6c603122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not directly discuss trends in politics, but it offers insights into the political strategies and campaigns of different parties during a specific election period (likely related to Scottish independence or similar referendums). The Liberal Democrats are positioning themselves as pragmatic on tax policy with potential for significant impact if they gain power. They aim to differentiate from Labour by not being seen solely as the party of the left and emphasize policies such as greater protection against problem debts, suggesting a focus on social welfare issues [1]. The Conservatives are criticized in Northern regions for their campaign tactics rather than policy trends themselves. There is no mention of blog readership or writing influencing political outcomes directly within the provided contexts [2][3][4].\n",
      "\n",
      "Therefore, based on this limited information from different sources: \n",
      "- The Liberal Democrats are focusing on tax policies and social welfare issues to differentiate themselves politically. (Source: Summary of BBC News Politics article)\n",
      "- There is a trend where the Conservatives' campaign tactics in certain regions may not be as effective, potentially influencing voter behavior [5]. \n",
      "- The role of blogs and internet readership seems to have some impact on political awareness but does not directly influence voting outcomes within this context. (Source: BBC News Tech article)\n",
      "\n",
      "==============================\n",
      "Q: What are some trends in sport?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c63d043a263437e812ce3a6fa6580bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not explicitly mention trends in sports, but it discusses various aspects of the gaming industry and its impact on athletes' careers. However, based on general knowledge outside this specific document, here are some current trends in sport that have been observed globally:\n",
      "\n",
      "1. Increased use of technology for performance analysis (e.g., wearable devices) [2] - This is not directly mentioned in the context but can be inferred as a broader industry trend affecting sports, including rugby and athletics like Paula Radcliffe's career. \n",
      "\n",
      "2. Growth of eSports: Competitive gaming has become increasingly popular worldwide [3] - While this is not directly mentioned in the context provided, it can be considered a significant trend within sports as gamers and athletes converge on digital platforms for competition. The ESPN deal referenced could potentially include coverage or development of eSports content given its association with gaming culture (though specifics are not detailed).\n",
      "\n",
      "3. Increased focus on mental health: Athletes' well-being is gaining more attention, including the impact of doping allegations and suspicions [4] - This trend aligns closely to Paula Radcliffe’s comments about athletes being treated as criminals when accused of drug use.\n",
      "\n",
      "Please note that these points are not directly sourced from the provided context but rather general knowledge on current sporting trends, which may or may not be reflected in this specific document's contents.\n"
     ]
    }
   ],
   "source": [
    "for q in [\n",
    "    \"What are some trends in tech?\",\n",
    "    \"What are some trends in politics?\",\n",
    "    \"What are some trends in sport?\"\n",
    "]:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Q:\", q)\n",
    "\n",
    "    hh = search(q, top_k=5)\n",
    "    answer = call_llm(build_prompt(q, hh))\n",
    "\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e06c1-d491-45fd-a25e-3233a6df4380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
